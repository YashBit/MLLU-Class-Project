{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUSTOMBERT-BERT-UCI-ZEROSHOT-SiFT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0a643f39f197486d90492fb458110723":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f676c6530fc540f2854a9d646bf5ea4a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c526151c2a854807a97271c6ae868786","IPY_MODEL_5d37143e6f2d4986b00fde60afa8693c"]}},"f676c6530fc540f2854a9d646bf5ea4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c526151c2a854807a97271c6ae868786":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a2ff25a03661445385764c784b6d294f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f92c1a3dc6148d1943ba5c2f2b5fdd1"}},"5d37143e6f2d4986b00fde60afa8693c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3100b10e6b77449ab4851290cb1c10fa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 113kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82888da8b5664760b7336ec83a9c67d6"}},"a2ff25a03661445385764c784b6d294f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f92c1a3dc6148d1943ba5c2f2b5fdd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3100b10e6b77449ab4851290cb1c10fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"82888da8b5664760b7336ec83a9c67d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3c7b98bc501d44dc9aef069c6d43901c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_146c9a3ff6d04e5887d46f270ce2c9d9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ae3392d4b3ed4b1b910aa8e52dee4252","IPY_MODEL_0c65e1cd6a8e4934b5bfda0d58014c0a"]}},"146c9a3ff6d04e5887d46f270ce2c9d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae3392d4b3ed4b1b910aa8e52dee4252":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_64bdbbe802c249bba546699870b7cddc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7abac79e1bc44a70b57b87503b24060f"}},"0c65e1cd6a8e4934b5bfda0d58014c0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e84b3716591f4e97b63f545208f297c1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 39.7B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ab655ebce3849d19989a02ee95d4497"}},"64bdbbe802c249bba546699870b7cddc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7abac79e1bc44a70b57b87503b24060f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e84b3716591f4e97b63f545208f297c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ab655ebce3849d19989a02ee95d4497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0bade09d7c6458c9dee3250ee36e275":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b674e7713403434dbe2fdf4b7cf5f1a4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_af9a2527a2464bdd8bad00bcc68ed1cb","IPY_MODEL_c601014b4d014dc9bd01e1cec2430b1e"]}},"b674e7713403434dbe2fdf4b7cf5f1a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af9a2527a2464bdd8bad00bcc68ed1cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_099ecbf1b9044910b34c2c4a227c13ed","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3a9ca885ba24e589443b2ebd71ff2f5"}},"c601014b4d014dc9bd01e1cec2430b1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ac31bf14bdc744529e6ed52a7a269882","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9055cc518b6743cfb82828553ef9a1ec"}},"099ecbf1b9044910b34c2c4a227c13ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f3a9ca885ba24e589443b2ebd71ff2f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac31bf14bdc744529e6ed52a7a269882":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9055cc518b6743cfb82828553ef9a1ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5069695051c0481b891000d516558f45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bb6e1324f651463d8493ac8a9a4e5afd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0ca4ad4ee95f4733b244e46fa17c94da","IPY_MODEL_8b9d7dfa246e46c8805d2b4ff31ae306"]}},"bb6e1324f651463d8493ac8a9a4e5afd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ca4ad4ee95f4733b244e46fa17c94da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2e7168b92b224c29af1342c96b81ab16","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29aa7bd45d094d9db05acab56b91e8b3"}},"8b9d7dfa246e46c8805d2b4ff31ae306":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_74e16ddae842466e81ae3ce04e907f21","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 1.19kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b35dd58124c0476e9663adf9c4e83ef5"}},"2e7168b92b224c29af1342c96b81ab16":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"29aa7bd45d094d9db05acab56b91e8b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74e16ddae842466e81ae3ce04e907f21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b35dd58124c0476e9663adf9c4e83ef5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a52c07dea4a248d1a248885619f954cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7f49600a2e0b45b0afba13e07e401a44","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2453f226180b4016bb683482e7dccd16","IPY_MODEL_08cc3c3e3cac439dac480322f40baf79"]}},"7f49600a2e0b45b0afba13e07e401a44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2453f226180b4016bb683482e7dccd16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8d73ed9115b54a1ba2701fe11ae13b07","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_990c174f7b364504aa7900b654e4f5df"}},"08cc3c3e3cac439dac480322f40baf79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_912c37d7021341a28cc60baa67565feb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:10&lt;00:00, 41.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b81fead34061448189a6d7cf34634558"}},"8d73ed9115b54a1ba2701fe11ae13b07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"990c174f7b364504aa7900b654e4f5df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"912c37d7021341a28cc60baa67565feb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b81fead34061448189a6d7cf34634558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"EKOTlwcmxmej"},"source":["# BERT Fine-Tuning on CoLA with SMART and SiFT\n","\n","This notebook is orginally created by Chris McCormick and Nick Ryan. We made changes for SiFT and SMART, as well as our custom BERT class."]},{"cell_type":"markdown","metadata":{"id":"jJKaoairpdRa"},"source":["##Data and Importing Modules "]},{"cell_type":"code","metadata":{"id":"DEfSbAA4QHas","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039684247,"user_tz":-330,"elapsed":7875,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"432de2bf-a12c-41b5-e7f9-aa4b1f7f9ac7"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oYsV4H8fCpZ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039688233,"user_tz":-330,"elapsed":11844,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"3711796b-e2b1-4737-c692-6a1f6b05c61c"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0NmMdkZO8R6q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039694595,"user_tz":-330,"elapsed":18182,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"4f20ff4a-8648-49bf-c1de-ae6f73af9dce"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 46.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 42.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5m6AnuFv0QXQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039698564,"user_tz":-330,"elapsed":22138,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"78129834-cb7e-4b10-b051-6d3ee8a5d8e2"},"source":["!pip install wget"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=33675dcc731ccb73729d8dae69055b87895ebe92008d0395deb8cbead81ee1f7\n","  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pMtmPMkBzrvs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039729319,"user_tz":-330,"elapsed":52883,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"00a94ab8-2599-4018-fdf9-e47db6b759f1"},"source":["#Adding the datasets to the collab file\n","\n","#First we mount the google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Yv-tNv20dnH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039823519,"user_tz":-330,"elapsed":5019,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"da5ea4be-66f5-49e4-9ca0-d825c4d7f8fe"},"source":["# with open('/content/drive/My Drive/Undergraduate/Courses/MLLU Project/Code/Baseline - Draft Proposal/yelp_labelled.txt', 'r') as f:\n","#   f.write('Successfully opened Yelp Labelled')\n","\n","with open('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/yelp_labelled.txt', 'r') as y:\n","  print(\"Successfully Opened Yelp\")\n","\n","\n","with open('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/amazon_cells_labelled.txt', 'r') as a:\n","  print(\"Successfully Opened Amazon Labelled\")\n","\n","\n","\n","with open('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/imdb_labelled.txt', 'r') as i:\n","  print(\"Successfully Opened IMDB Labelled\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Successfully Opened Yelp\n","Successfully Opened Amazon Labelled\n","Successfully Opened IMDB Labelled\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_UkeC7SG2krJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039824347,"user_tz":-330,"elapsed":5082,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"e2e7eee1-87c8-4c0f-f7d3-8c006d0cc9e5"},"source":["#Checking on the Yelp Dataframe\n","import pandas as pd\n","df1_yelp = pd.read_csv('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/yelp_labelled.txt', delimiter = '\\t', delim_whitespace= False, names = ('Sentence', 'Label'))\n","df1_yelp.head()\n","\n","print(df1_yelp.info())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Sentence  1000 non-null   object\n"," 1   Label     1000 non-null   int64 \n","dtypes: int64(1), object(1)\n","memory usage: 15.8+ KB\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-0v9aKSCWVg","executionInfo":{"status":"ok","timestamp":1621039825285,"user_tz":-330,"elapsed":5783,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"a30c592d-3410-477b-e869-ce7a639126f4"},"source":["\n","#Similarly for the dataframes for Amazon and IMDB\n","df1_amazon = pd.read_csv('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/amazon_cells_labelled.txt', delimiter = '\\t', delim_whitespace= False, names = ('Sentence', 'Label'))\n","df1_amazon.head()\n","print(df1_amazon.info())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Sentence  1000 non-null   object\n"," 1   Label     1000 non-null   int64 \n","dtypes: int64(1), object(1)\n","memory usage: 15.8+ KB\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wPamOpWCYhk","executionInfo":{"status":"ok","timestamp":1621039825286,"user_tz":-330,"elapsed":5622,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"f7ecc1bb-9962-4bf0-e36b-4c3aeb144a00"},"source":["df1_imdb = pd.read_csv('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/imdb_labelled.txt', delimiter = '\\t', delim_whitespace= False, names = ('Sentence', 'Label'))\n","df1_imdb.head()\n","print(df1_imdb.info())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 748 entries, 0 to 747\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Sentence  748 non-null    object\n"," 1   Label     748 non-null    int64 \n","dtypes: int64(1), object(1)\n","memory usage: 11.8+ KB\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"blqIvQaQncdJ","colab":{"base_uri":"https://localhost:8080/","height":512},"executionInfo":{"status":"ok","timestamp":1621039825287,"user_tz":-330,"elapsed":5419,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"98fe912e-6742-4551-ac7b-ede52084fada"},"source":["\n","combinedDF = pd.concat([df1_imdb, df1_amazon, df1_yelp], axis = 0, join = 'inner')\n","combinedDF.head()\n","combinedDF.info()\n","combinedDF.head(10)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 2748 entries, 0 to 999\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Sentence  2748 non-null   object\n"," 1   Label     2748 non-null   int64 \n","dtypes: int64(1), object(1)\n","memory usage: 64.4+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A very, very, very slow-moving, aimless movie ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Not sure who was more lost - the flat characte...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Attempting artiness with black &amp; white and cle...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Very little music or anything to speak of.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The best scene in the movie was when Gerardo i...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>The rest of the movie lacks art, charm, meanin...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Wasted two hours.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Saw the movie today and thought it was a good ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>A bit predictable.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Loved the casting of Jimmy Buffet as the scien...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Sentence  Label\n","0  A very, very, very slow-moving, aimless movie ...      0\n","1  Not sure who was more lost - the flat characte...      0\n","2  Attempting artiness with black & white and cle...      0\n","3       Very little music or anything to speak of.        0\n","4  The best scene in the movie was when Gerardo i...      1\n","5  The rest of the movie lacks art, charm, meanin...      0\n","6                                Wasted two hours.        0\n","7  Saw the movie today and thought it was a good ...      1\n","8                               A bit predictable.        0\n","9  Loved the casting of Jimmy Buffet as the scien...      1"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"qaDK92pbCfZZ","executionInfo":{"status":"ok","timestamp":1621039825288,"user_tz":-330,"elapsed":5159,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"517047cb-0e72-4682-f2ee-4c95585ed2db"},"source":["combinedDF = combinedDF.sample(frac = 1).reset_index(drop = True)\n","combinedDF = combinedDF.dropna()\n","combinedDF.info()\n","combinedDF.head(10)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 2748 entries, 0 to 2747\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Sentence  2748 non-null   object\n"," 1   Label     2748 non-null   int64 \n","dtypes: int64(1), object(1)\n","memory usage: 64.4+ KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Your servers suck, wait, correction, our serve...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Magical Help.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The cocktails are all handmade and delicious.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I recently had problems where I could not stay...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>This results in the phone being either stuck a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Essentially you can forget Microsoft's tech su...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I came over from Verizon because cingulair has...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>I had a pretty satifying experience.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>The bathrooms are clean and the place itself i...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>There was a few pathetic attempts to give the ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Sentence  Label\n","0  Your servers suck, wait, correction, our serve...      0\n","1                                      Magical Help.      1\n","2      The cocktails are all handmade and delicious.      1\n","3  I recently had problems where I could not stay...      0\n","4  This results in the phone being either stuck a...      0\n","5  Essentially you can forget Microsoft's tech su...      0\n","6  I came over from Verizon because cingulair has...      0\n","7               I had a pretty satifying experience.      1\n","8  The bathrooms are clean and the place itself i...      1\n","9  There was a few pathetic attempts to give the ...      0"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"GuE5BqICAne2","executionInfo":{"status":"ok","timestamp":1621039825289,"user_tz":-330,"elapsed":4916,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["df1_twitter = pd.read_csv('/content/drive/MyDrive/Undergraduate /Courses /MLLU Project/Code/MLLU : Final Paper Submission Code/train.csv', names = ('Label', 'Sentence'))\n","df1_twitter = df1_twitter.iloc[1:4000]\n","\n","df1_twitter['Label'] = df1_twitter['Label'].astype(int, errors = 'raise')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XcD3h68Ck7v","executionInfo":{"status":"ok","timestamp":1621039825289,"user_tz":-330,"elapsed":4692,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"f4e0ddf6-8e52-4fc3-f50d-85a8c48f2ac6"},"source":["df1_twitter.head()\n","df1_twitter.info()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 3999 entries, 1 to 3999\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Label     3999 non-null   int64 \n"," 1   Sentence  3999 non-null   object\n","dtypes: int64(1), object(1)\n","memory usage: 93.7+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykLveVUNCneG","executionInfo":{"status":"ok","timestamp":1621039825930,"user_tz":-330,"elapsed":5144,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"3c4c343e-1f97-4e24-f256-3382fcd3bf11"},"source":["df1_twitter.head(10)\n","sentences = df1_twitter.Sentence.values\n","labels = df1_twitter.Label.values\n","\n","print(type(labels[0]))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["<class 'numpy.int64'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZvoRQlgOCq9a","executionInfo":{"status":"ok","timestamp":1621039825931,"user_tz":-330,"elapsed":4995,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":[""],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EFSJzwI5pujc"},"source":["## Tokenization and DataLoader"]},{"cell_type":"code","metadata":{"id":"Z474sSC6oe7A","colab":{"base_uri":"https://localhost:8080/","height":181,"referenced_widgets":["0a643f39f197486d90492fb458110723","f676c6530fc540f2854a9d646bf5ea4a","c526151c2a854807a97271c6ae868786","5d37143e6f2d4986b00fde60afa8693c","a2ff25a03661445385764c784b6d294f","3f92c1a3dc6148d1943ba5c2f2b5fdd1","3100b10e6b77449ab4851290cb1c10fa","82888da8b5664760b7336ec83a9c67d6","3c7b98bc501d44dc9aef069c6d43901c","146c9a3ff6d04e5887d46f270ce2c9d9","ae3392d4b3ed4b1b910aa8e52dee4252","0c65e1cd6a8e4934b5bfda0d58014c0a","64bdbbe802c249bba546699870b7cddc","7abac79e1bc44a70b57b87503b24060f","e84b3716591f4e97b63f545208f297c1","2ab655ebce3849d19989a02ee95d4497","f0bade09d7c6458c9dee3250ee36e275","b674e7713403434dbe2fdf4b7cf5f1a4","af9a2527a2464bdd8bad00bcc68ed1cb","c601014b4d014dc9bd01e1cec2430b1e","099ecbf1b9044910b34c2c4a227c13ed","f3a9ca885ba24e589443b2ebd71ff2f5","ac31bf14bdc744529e6ed52a7a269882","9055cc518b6743cfb82828553ef9a1ec"]},"executionInfo":{"status":"ok","timestamp":1621039828532,"user_tz":-330,"elapsed":7333,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"8d9c1c0c-fcd9-45e6-ebc0-7b98ff751ef3"},"source":["from transformers import BertTokenizer\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a643f39f197486d90492fb458110723","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c7b98bc501d44dc9aef069c6d43901c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0bade09d7c6458c9dee3250ee36e275","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dLIbudgfh6F0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039828534,"user_tz":-330,"elapsed":7099,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"3f4d1408-57db-43ce-e067-21582a508d64"},"source":["print(' Original: ', sentences[0])\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":22,"outputs":[{"output_type":"stream","text":[" Original:   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n","Tokenized:  ['@', 'user', 'when', 'a', 'father', 'is', 'dysfunction', '##al', 'and', 'is', 'so', 'selfish', 'he', 'drag', '##s', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n","Token IDs:  [1030, 5310, 2043, 1037, 2269, 2003, 28466, 2389, 1998, 2003, 2061, 14337, 2002, 8011, 2015, 2010, 4268, 2046, 2010, 28466, 1012, 1001, 2448]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cKsH2sU0OCQA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039830469,"user_tz":-330,"elapsed":8924,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"ac6207a3-07c0-4c95-952d-d9c052f232b0"},"source":["max_len = 0\n","\n","# For every sentence...\n","for sent in sentences:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","\n","print(len(sentences))\n","print('Max sentence length: ', max_len)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["3999\n","Max sentence length:  76\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2bBdb3pt8LuQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039832795,"user_tz":-330,"elapsed":11165,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"21c82a68-87d8-4157-a78c-3e4bedd073ca"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 32,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n","Token IDs: tensor([  101,  1030,  5310,  2043,  1037,  2269,  2003, 28466,  2389,  1998,\n","         2003,  2061, 14337,  2002,  8011,  2015,  2010,  4268,  2046,  2010,\n","        28466,  1012,  1001,  2448,   102,     0,     0,     0,     0,     0,\n","            0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GEgLpFVlo1Z-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039832797,"user_tz":-330,"elapsed":11028,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"c74c97bc-4149-40ae-bd68-34f180af1df9"},"source":["\n","from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","print(type(input_ids))\n","print(type(attention_masks))\n","print(type(labels))\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","3,599 training samples\n","  400 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XGUqOCtgqGhP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621039832803,"user_tz":-330,"elapsed":10807,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"5bcea9a1-f483-4dca-c92a-6470ac148f84"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. \n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","print(\"hi\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["hi\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"73S4P4SMp6hX"},"source":["## Custom Bert Class and Initialization"]},{"cell_type":"code","metadata":{"id":"UOteWAT-Adqx","executionInfo":{"status":"ok","timestamp":1621039832805,"user_tz":-330,"elapsed":10589,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n","from transformers.models.bert.modeling_bert import BertEmbeddings,BertEncoder,BertPooler\n","from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n","\n","import torch\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","\n","class CustomBertForClassification(BertForSequenceClassification):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        #self.bert = BertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = False)\n","        self.embeddings = self.bert.embeddings\n","        self.encoder = self.bert.encoder\n","        self.pooler = self.bert.pooler\n","\n","    def embed(self, input_ids=None, \n","                    token_type_ids=None, \n","                    position_ids=None, \n","                    inputs_embeds=None, \n","                    past_key_values_length=0):\n","        # See: BERTModel.forward\n","        return self.embeddings(\n","            input_ids=input_ids,\n","            position_ids=position_ids,\n","            token_type_ids=token_type_ids,\n","            inputs_embeds=inputs_embeds,\n","            past_key_values_length=past_key_values_length\n","        )\n","    \n","    def predict(self,embedding_output,\n","                extended_attention_mask=None,\n","                head_mask=None,\n","                encoder_hidden_states=None,\n","                encoder_extended_attention_mask=None,\n","                past_key_values=None,\n","                use_cache=None,\n","                output_attentions=None,\n","                output_hidden_states=None,\n","                return_dict=True):\n","      # See: BERTModel.forward \n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            attention_mask=extended_attention_mask,\n","            head_mask=head_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_extended_attention_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n","        \n","        bert_output = BaseModelOutputWithPoolingAndCrossAttentions(\n","                    last_hidden_state=sequence_output,\n","                    pooler_output=pooled_output,\n","                    past_key_values=encoder_outputs.past_key_values,\n","                    hidden_states=encoder_outputs.hidden_states,\n","                    attentions=encoder_outputs.attentions,\n","                    cross_attentions=encoder_outputs.cross_attentions,\n","                )\n","\n","        pooled_output = bert_output[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        \n","        return logits\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5069695051c0481b891000d516558f45","bb6e1324f651463d8493ac8a9a4e5afd","0ca4ad4ee95f4733b244e46fa17c94da","8b9d7dfa246e46c8805d2b4ff31ae306","2e7168b92b224c29af1342c96b81ab16","29aa7bd45d094d9db05acab56b91e8b3","74e16ddae842466e81ae3ce04e907f21","b35dd58124c0476e9663adf9c4e83ef5","a52c07dea4a248d1a248885619f954cb","7f49600a2e0b45b0afba13e07e401a44","2453f226180b4016bb683482e7dccd16","08cc3c3e3cac439dac480322f40baf79","8d73ed9115b54a1ba2701fe11ae13b07","990c174f7b364504aa7900b654e4f5df","912c37d7021341a28cc60baa67565feb","b81fead34061448189a6d7cf34634558"]},"id":"IdNBO5qk2-i_","collapsed":true,"executionInfo":{"status":"ok","timestamp":1621039856289,"user_tz":-330,"elapsed":33907,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"762277e2-9525-43ab-e6ca-b2b01ff85271"},"source":["#@title\n","model = CustomBertForClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels = 2,\n","    output_attentions = False, \n","    output_hidden_states = False, \n",")\n","\n","model.cuda()"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5069695051c0481b891000d516558f45","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a52c07dea4a248d1a248885619f954cb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing CustomBertForClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomBertForClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomBertForClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'classifier.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'classifier.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["CustomBertForClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"hmSpMRD5qaqE"},"source":["##Noise Function"]},{"cell_type":"code","metadata":{"id":"pG5DszcpDAjw","executionInfo":{"status":"ok","timestamp":1621039962258,"user_tz":-330,"elapsed":1339,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["from torch.nn import LayerNorm\n","import torch.nn.functional as F\n","\n","def normalize_embed(embed):\n","    embed_mean = torch.mean(embed,dim=(1,2))\n","    embed_std = torch.std(embed, dim=(1,2))\n","\n","    embed_clone = torch.clone(embed)\n","\n","    for i in range(0,embed_clone.size()[0]):\n","        # embed_clone[i] = torch.div(torch.sub(embed_clone[i],embed_mean[i]),embed_std[i])\n","        embed_clone[i] = (embed_clone[i] - embed_mean[i]) / embed_std[i]\n","    return embed_clone, embed_mean, embed_std\n","\n","def denormalize_embed(embed, embed_mean, embed_std):\n","    for i in range(0,embed.size()[0]):\n","        # embed[i] = (embed[i] - embed_mean[i]) / embed_std[i]\n","        embed[i] = (embed[i] * embed_std[i]) + embed_mean[i]\n","    return embed \n","\n","def stable_kl(logit, target, epsilon=1e-6, reduce=True):\n","    logit = logit.view(-1, logit.size(-1)).float()\n","    target = target.view(-1, target.size(-1)).float()\n","    bs = logit.size(0)\n","    p = F.log_softmax(logit, 1).exp()\n","    y = F.log_softmax(target, 1).exp()\n","    rp = -(1.0/(p + epsilon) -1 + epsilon).detach().log()\n","    ry = -(1.0/(y + epsilon) -1 + epsilon).detach().log()\n","    if reduce:\n","        return (p* (rp- ry) * 2).sum() / bs\n","    else:\n","        return (p* (rp- ry) * 2).sum()\n","\n","def _norm_grad(grad, epsilon = 1e-6, eff_grad=None, sentence_level=False):\n","        if sentence_level:\n","            direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n","        else:\n","            direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n","            eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n","        return direction, eff_direction\n","\n","def noise(embed, model,step_size, normalize=False, k=1, mean=0, std=0.01):  ## Not including mask in the noise, so it means no mask as input for predict, should be a problem\n","    if normalize == True:\n","        logits = model.predict(embed)#,attention_mask)\n","        # LNorm = LayerNorm(embed.size(),elementwise_affine=False)\n","        # normalized_embed = LNorm(embed)\n","        normalized_embed, embed_mean, embed_std = normalize_embed(embed)\n","\n","        noise = torch.normal(mean=0, std=0.01,size=(normalized_embed.size()[0],normalized_embed.size()[1],normalized_embed.size()[2]))\n","        noise = noise.to(device)\n","        noise.requires_grad_()\n","        noised_normalized_embeddings = normalized_embed+noise\n","        adv_logits = model.predict(noised_normalized_embeddings)#,attention_mask)\n","        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n","        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n","        norm = delta_grad.norm()\n","        # if (torch.isnan(norm) or torch.isinf(norm)):\n","        #     return 0\n","        eff_delta_grad = delta_grad * step_size\n","        delta_grad = noise + delta_grad * step_size\n","        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n","        noise = noise.detach()\n","        noised_normalized_embeddings = normalized_embed+noise\n","        denormalize_noised_embed = denormalize_embed(noised_normalized_embeddings,embed_mean, embed_std) \n","        return denormalize_noised_embed\n","    else:\n","        logits = model.predict(embed)#,attention_mask)\n","        noise = torch.normal(mean=0, std=0.01,size=(embed.size()[0],embed.size()[1],embed.size()[2]))\n","        noise = noise.to(device)\n","        noise.requires_grad_()\n","        noised_embeddings = embed+noise\n","        adv_logits = model.predict(noised_embeddings)#,attention_mask)\n","        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n","        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n","        norm = delta_grad.norm()\n","        # if (torch.isnan(norm) or torch.isinf(norm)):\n","        #     return 0\n","        eff_delta_grad = delta_grad * step_size\n","        delta_grad = noise + delta_grad * step_size\n","        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n","        noise = noise.detach()\n","        noised_embeddings = embed+noise\n","        return noised_embeddings"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bunW4qF4qSyZ"},"source":["## Optimizer, Scheduler, and Some Other Training Prep"]},{"cell_type":"code","metadata":{"id":"GLs72DuMODJO","executionInfo":{"status":"ok","timestamp":1621039965215,"user_tz":-330,"elapsed":1388,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["#@title\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8 \n","                )"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"-p0upAhhRiIx","executionInfo":{"status":"ok","timestamp":1621039965217,"user_tz":-330,"elapsed":799,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cQNvaZ9bnyy","executionInfo":{"status":"ok","timestamp":1621039967366,"user_tz":-330,"elapsed":1761,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["#@title\n","import numpy as np\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpt6tR83keZD","executionInfo":{"status":"ok","timestamp":1621039967369,"user_tz":-330,"elapsed":804,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["#@title\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScjvBSBfHtBc","executionInfo":{"status":"ok","timestamp":1621039968713,"user_tz":-330,"elapsed":881,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["MODE = \"SIFT\""],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCSpuOXLqor-"},"source":["##Training Loop with Validation"]},{"cell_type":"code","metadata":{"id":"6J-FYdx6nFE_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621040197546,"user_tz":-330,"elapsed":225738,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"98f3c578-de2e-4e8c-de44-e481de7df0da"},"source":["import random\n","import numpy as np\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","    total_train_loss = 0\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        embed = model.embed(input_ids = b_input_ids)\n","        preds = model.predict(embedding_output = embed)#,extended_attention_mask=b_input_mask)   <- Didn't use mask at all, which should be a problem\n","        loss_fct = CrossEntropyLoss()\n","        regular_loss = loss_fct(preds.view(-1,2), b_labels.view(-1))\n","        loss_list = [regular_loss]\n","        if MODE in [\"SMART-adv-only\", \"SIFT\"]:\n","          normalise = True if MODE == \"SIFT\" else False\n","          noised_embeddings = noise(embed, model, 1e-3, normalize=normalise, k=1)\n","          adv_logits = model.predict(embedding_output = noised_embeddings)#,extended_attention_mask = b_input_mask)   <- Didn't use mask at all, which should be a problem\n","\n","          adv_loss = stable_kl(preds.view(-1,2), adv_logits.view(-1,2))\n","          loss_list.append(adv_loss)\n","        loss = sum(loss_list)\n","        # END MODEL\n","        total_train_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    model.eval()\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        with torch.no_grad():        \n","\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        total_eval_loss += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)   \n","\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    113.    Elapsed: 0:00:18.\n","  Batch    80  of    113.    Elapsed: 0:00:37.\n","\n","  Average training loss: 0.24\n","  Training epcoh took: 0:00:53\n","\n","Running Validation...\n","  Accuracy: 0.94\n","  Validation Loss: 0.17\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    113.    Elapsed: 0:00:19.\n","  Batch    80  of    113.    Elapsed: 0:00:39.\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:00:55\n","\n","Running Validation...\n","  Accuracy: 0.97\n","  Validation Loss: 0.13\n","  Validation took: 0:00:01\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    113.    Elapsed: 0:00:20.\n","  Batch    80  of    113.    Elapsed: 0:00:40.\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:00:57\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation Loss: 0.14\n","  Validation took: 0:00:01\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    113.    Elapsed: 0:00:20.\n","  Batch    80  of    113.    Elapsed: 0:00:41.\n","\n","  Average training loss: 0.07\n","  Training epcoh took: 0:00:57\n","\n","Running Validation...\n","  Accuracy: 0.97\n","  Validation Loss: 0.14\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:03:45 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VQTvJ1vRP7u4"},"source":["Let's view the summary of the training process."]},{"cell_type":"code","metadata":{"id":"6O_NbXFGMukX","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1621040257044,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"20df2ab5-015d-4d2b-fb63-e8717eba0185"},"source":["\n","import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.24</td>\n","      <td>0.17</td>\n","      <td>0.94</td>\n","      <td>0:00:53</td>\n","      <td>0:00:01</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.13</td>\n","      <td>0.13</td>\n","      <td>0.97</td>\n","      <td>0:00:55</td>\n","      <td>0:00:01</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.09</td>\n","      <td>0.14</td>\n","      <td>0.96</td>\n","      <td>0:00:57</td>\n","      <td>0:00:01</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.07</td>\n","      <td>0.14</td>\n","      <td>0.97</td>\n","      <td>0:00:57</td>\n","      <td>0:00:01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.24         0.17           0.94       0:00:53         0:00:01\n","2               0.13         0.13           0.97       0:00:55         0:00:01\n","3               0.09         0.14           0.96       0:00:57         0:00:01\n","4               0.07         0.14           0.97       0:00:57         0:00:01"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"68xreA9JAmG5","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"ok","timestamp":1621040261016,"user_tz":-330,"elapsed":2069,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"2bc3395f-3089-43af-af87-99dd35fc050f"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"execution_count":47,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zWeYo84kDYZM","executionInfo":{"status":"ok","timestamp":1621040317375,"user_tz":-330,"elapsed":2994,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"50fde4fc-a8ea-49ae-88e7-3d26e29957be"},"source":["total_test_loss = []\n","total_test_accuracy = []\n","\n","\n","\n","for batch in validation_dataloader:\n","\n","        \n","  # Unpack this training batch from our dataloader. \n","  #\n","  # As we unpack the batch, we'll also copy each tensor to the GPU using \n","  # the `to` method.\n","  #\n","  # `batch` contains three pytorch tensors:\n","  #   [0]: input ids \n","  #   [1]: attention masks\n","  #   [2]: labels \n","  b_input_ids = batch[0].to(device)\n","  b_input_mask = batch[1].to(device)\n","  b_labels = batch[2].to(device)\n","        \n","  # Tell pytorch not to bother with constructing the compute graph during\n","  # the forward pass, since this is only needed for backprop (training).\n","  #with torch.no_grad():        \n","\n","  # Forward pass, calculate logit predictions.\n","  # token_type_ids is the same as the \"segment ids\", which \n","  # differentiates sentence 1 and 2 in 2-sentence tasks.\n","  result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","  loss = result.loss\n","  logits = result.logits\n","        \n","            \n","        # Accumulate the validation loss.\n","  total_test_loss = loss.item()\n","\n","        # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","  total_test_accuracy = flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","print(\"The total test accuracy is: \")\n","print(\"  Accuracy: {0:.5f}\".format(total_test_accuracy))\n","print(\"Testing complete!\")"],"execution_count":51,"outputs":[{"output_type":"stream","text":["The total test accuracy is: \n","  Accuracy: 1.00000\n","Testing complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mkyubuJSOzg3"},"source":["##Performance On Test Set -> UCI SENTIMENT ANALYSIS"]},{"cell_type":"code","metadata":{"id":"SVuaPBtNDlj1","executionInfo":{"status":"ok","timestamp":1621040321602,"user_tz":-330,"elapsed":1249,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}}},"source":["\n","combinedDF['Label'] = combinedDF['Label'].astype(int, errors = 'raise')\n","sentences3 = combinedDF.Sentence.values\n","labels3 = combinedDF.Label.values"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESDPcLjNDmKi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621040324779,"user_tz":-330,"elapsed":2624,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"473b4633-2ca1-4e13-9521-6409c1a33c08"},"source":["max_len3 = 0\n","\n","# For every sentence...\n","for sent in sentences3:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids3 = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len3 = max(max_len3, len(input_ids3))\n","\n","print('Max sentence length: ', max_len3)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["Max sentence length:  1790\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NOzFdf7EDn3j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621040328627,"user_tz":-330,"elapsed":2313,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"4511f425-55ae-4a0b-9ef1-710dc9285127"},"source":["input_ids3 = []\n","attention_masks3 = []\n","\n","# For every sentence...\n","for sent in sentences3:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict3 = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids3.append(encoded_dict3['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks3.append(encoded_dict3['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids3 = torch.cat(input_ids3, dim=0)\n","attention_masks3 = torch.cat(attention_masks3, dim=0)\n","labels3 = torch.tensor(labels3)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences3[0])\n","print('Token IDs:', input_ids3[0])\n","\n","\n","print(len(sentences3))\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:  Your servers suck, wait, correction, our server Heimer sucked.\n","Token IDs: tensor([  101,  2115, 14903, 11891,  1010,  3524,  1010, 18140,  1010,  2256,\n","         8241,  2002, 14428,  2099,  8631,  1012,   102,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0])\n","2748\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aEFlEYwZDp_D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621040331605,"user_tz":-330,"elapsed":1417,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"65db6021-3e1a-478b-dfbd-f9f639baa060"},"source":["test_dataset = TensorDataset(input_ids3, attention_masks3, labels3)\n","\n","print(type(input_ids3))        \n","print(type(attention_masks3))\n","print(type(labels3))\n","\n","\n","\n","test_dataloader = DataLoader(        \n","            test_dataset, # The validation samples.\n","            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","            batch_size = 32 \n","        )\n","\n","\n","print(len(test_dataset))"],"execution_count":55,"outputs":[{"output_type":"stream","text":["<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","<class 'torch.Tensor'>\n","2748\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qAS6R3FEDrys","colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"status":"ok","timestamp":1621040333956,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"7fcdd5cf-60a1-4037-94e1-f73f6fd4cc6b"},"source":["torch.cuda.memory_summary(device=None, abbreviated=False)\n"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    2374 MB |    4432 MB |    3863 GB |    3861 GB |\\n|       from large pool |    2316 MB |    4333 MB |    3814 GB |    3812 GB |\\n|       from small pool |      57 MB |     138 MB |      48 GB |      48 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    2374 MB |    4432 MB |    3863 GB |    3861 GB |\\n|       from large pool |    2316 MB |    4333 MB |    3814 GB |    3812 GB |\\n|       from small pool |      57 MB |     138 MB |      48 GB |      48 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    4516 MB |    4516 MB |    4516 MB |       0 B  |\\n|       from large pool |    4370 MB |    4370 MB |    4370 MB |       0 B  |\\n|       from small pool |     146 MB |     146 MB |     146 MB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  661467 KB |  680923 KB |    3723 GB |    3722 GB |\\n|       from large pool |  626176 KB |  638912 KB |    3648 GB |    3647 GB |\\n|       from small pool |   35291 KB |   64116 KB |      74 GB |      74 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1275    |    1877    |    1703 K  |    1702 K  |\\n|       from large pool |     544    |     933    |     953 K  |     952 K  |\\n|       from small pool |     731    |    1037    |     750 K  |     749 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1275    |    1877    |    1703 K  |    1702 K  |\\n|       from large pool |     544    |     933    |     953 K  |     952 K  |\\n|       from small pool |     731    |    1037    |     750 K  |     749 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     311    |     311    |     311    |       0    |\\n|       from large pool |     238    |     238    |     238    |       0    |\\n|       from small pool |      73    |      73    |      73    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     148    |     205    |    1054 K  |    1054 K  |\\n|       from large pool |      73    |     101    |     697 K  |     697 K  |\\n|       from small pool |      75    |     113    |     357 K  |     357 K  |\\n|===========================================================================|\\n'"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"OPQD0agDDtRH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621040349283,"user_tz":-330,"elapsed":12831,"user":{"displayName":"Yash Bharti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gild8Odm0H-eBe0PGDUvXNharCipuIl3350SiqJBw=s64","userId":"02175383882112157218"}},"outputId":"3e96babc-e8b0-4010-aae8-5dd751a49339"},"source":["total_test_loss = []\n","total_test_accuracy = []\n","\n","\n","\n","for batch in test_dataloader:\n","\n","        \n","  # Unpack this training batch from our dataloader. \n","  #\n","  # As we unpack the batch, we'll also copy each tensor to the GPU using \n","  # the `to` method.\n","  #\n","  # `batch` contains three pytorch tensors:\n","  #   [0]: input ids \n","  #   [1]: attention masks\n","  #   [2]: labels \n","  b_input_ids = batch[0].to(device)\n","  b_input_mask = batch[1].to(device)\n","  b_labels = batch[2].to(device)\n","        \n","  # Tell pytorch not to bother with constructing the compute graph during\n","  # the forward pass, since this is only needed for backprop (training).\n","  #with torch.no_grad():        \n","\n","  # Forward pass, calculate logit predictions.\n","  # token_type_ids is the same as the \"segment ids\", which \n","  # differentiates sentence 1 and 2 in 2-sentence tasks.\n","  result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","  loss = result.loss\n","  logits = result.logits\n","        \n","            \n","        # Accumulate the validation loss.\n","  total_test_loss = loss.item()\n","\n","        # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","  total_test_accuracy = flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","print(\"The total test accuracy is: \")\n","print(total_test_accuracy)\n","print(\"Testing complete!\")"],"execution_count":57,"outputs":[{"output_type":"stream","text":["The total test accuracy is: \n","0.6071428571428571\n","Testing complete!\n"],"name":"stdout"}]}]}